{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to import PySpark and check the version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 13:50:01 WARN Utils: Your hostname, AI-CJB-LAP-460 resolves to a loopback address: 127.0.1.1; using 192.168.1.165 instead (on interface wlp0s20f3)\n",
      "24/09/23 13:50:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/23 13:50:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark 101 Exercises\").getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How to convert the index of a PySpark DataFrame into a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 14:07:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 14:07:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 14:07:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|Index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    1|\n",
      "|    Bob|    2|    2|\n",
      "|Charlie|    3|    3|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Convert Index to Column\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([\n",
    "    (\"Alice\", 1),\n",
    "    (\"Bob\", 2),\n",
    "    (\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "win= Window.orderBy(\"Name\") \n",
    "df_with_index = df.withColumn(\"Index\", row_number().over(win))\n",
    "df_with_index.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How to combine many lists to form a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "|   d|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "rdd=spark.sparkContext.parallelize(list(zip(list1,list2)))\n",
    "df=rdd.toDF([\"col1\",\"col2\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to get the items of list A not present in list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "rdd1=spark.sparkContext.parallelize(list_A)\n",
    "rdd2=spark.sparkContext.parallelize(list_B)\n",
    "res=rdd1.subtract(rdd2)\n",
    "res.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How to get the items not common to both list A and list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 6, 7, 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "rdd1=spark.sparkContext.parallelize(list_A)\n",
    "rdd2=spark.sparkContext.parallelize(list_B)\n",
    "res1=rdd1.subtract(rdd2)\n",
    "res2=rdd2.subtract(rdd1)\n",
    "res=res1.union(res2)\n",
    "res.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n",
      "Min:  10.0\n",
      "25th percentile:  20.0\n",
      "Median:  30.0\n",
      "75th percentile:  50.0\n",
      "Max:  86.0\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How to get frequency counts of unique items of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|count|\n",
      "+----+-----+\n",
      "|John|    2|\n",
      "|Mary|    1|\n",
      "| Bob|    3|\n",
      "| Sam|    1|\n",
      "+----+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "| Engineer|    4|\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.groupBy(\"name\").count().show()\n",
    "df.groupBy(\"job\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    Other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, when\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "top_2 = df.groupBy('job').count()\\\n",
    "    .orderBy('count', ascending=False)\\\n",
    "        .limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
    "df = df.withColumn('job', when(col('job')\\\n",
    ".isin(top_2), col('job')).otherwise('Other'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How to Drop rows with NA values specific to a particular column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B|    3| 456|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "df1=df.dropna(subset=[\"Value\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "df1 = df.toDF(*new_names)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. How to bin a numeric list to 10 groups of equal size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binned DataFrame:\n",
      "+-------------------+------+\n",
      "|             values|bucket|\n",
      "+-------------------+------+\n",
      "|  0.619189370225301|     7|\n",
      "| 0.5096018842446481|     6|\n",
      "| 0.8325259388871524|     9|\n",
      "|0.26322809041172357|     3|\n",
      "| 0.6702867696264135|     7|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql import functions as F\n",
    "num_items = 100\n",
    "df = spark.range(num_items).select\\\n",
    "    (rand(seed=42).alias(\"values\"))\n",
    "min_value = df.agg(F.min(\"values\")).first()[0]\n",
    "max_value = df.agg(F.max(\"values\")).first()[0]\n",
    "bin_size = (max_value - min_value) / 10\n",
    "binned_df = df.withColumn(\n",
    "    \"bucket\",\n",
    "    (F.col(\"values\") - min_value) / bin_size\n",
    ")\n",
    "binned_df = binned_df.withColumn(\"bucket\", F.ceil(F.col(\"bucket\")))\n",
    "print(\"Binned DataFrame:\")\n",
    "binned_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. How to create contigency table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+---+\n",
      "|category1_category2|  X|  Y|\n",
      "+-------------------+---+---+\n",
      "|                  B|  1|  1|\n",
      "|                  C|  2|  1|\n",
      "|                  A|  2|  1|\n",
      "+-------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "df.crosstab(\"category1\", \"category2\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. How to find the numbers that are multiples of 3 from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+\n",
      "| id|random|is_multiple_of_3|\n",
      "+---+------+----------------+\n",
      "|  0|     7|              no|\n",
      "|  1|     9|             yes|\n",
      "|  2|     8|              no|\n",
      "|  3|     8|              no|\n",
      "|  4|     3|             yes|\n",
      "|  5|     1|              no|\n",
      "|  6|     7|              no|\n",
      "|  7|     4|              no|\n",
      "|  8|     5|              no|\n",
      "|  9|     1|              no|\n",
      "+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "df = spark.range(10)\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "df1=df.withColumn(\"is_multiple_of_3\", when(col(\"random\") % 3 == 0,\"yes\").otherwise('no'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. How to extract items at given positions from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|random|index|\n",
      "+---+------+-----+\n",
      "|  0|     7|    0|\n",
      "|  1|     9|    1|\n",
      "|  2|     8|    2|\n",
      "|  3|     8|    3|\n",
      "|  4|     3|    4|\n",
      "|  5|     1|    5|\n",
      "|  6|     7|    6|\n",
      "|  7|     4|    7|\n",
      "|  8|     5|    8|\n",
      "|  9|     1|    9|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "| id|random|index|\n",
      "+---+------+-----+\n",
      "|  0|     7|    0|\n",
      "|  4|     3|    4|\n",
      "|  5|     1|    5|\n",
      "|  8|     5|    8|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:21:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "pos = [0, 4, 8, 5]\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "pos = [0, 4, 8, 5]\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Filter the DataFrame based on the specified positions\n",
    "df_filtered = df.filter(df.index.isin(pos))\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. How to stack two DataFrames vertically ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_A.union(df_B).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. How to compute the mean squared error on a truth and predicted columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  MSE|\n",
      "+-----+\n",
      "|116.8|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
    "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
    "df.select(F.mean((F.col(\"actual\") - F.col(\"predicted\")) ** 2).alias(\"MSE\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. How to convert the first character of each element in a series to uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| John|\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df = df.withColumn(\"name\", initcap(df.name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. How to compute summary statistics for all columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 15:44:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+-----------------+\n",
      "|summary|  name|              age|           salary|\n",
      "+-------+------+-----------------+-----------------+\n",
      "|  count|     5|                5|                5|\n",
      "|   mean|  NULL|             32.4|          66000.0|\n",
      "| stddev|  NULL|3.209361307176242|9617.692030835675|\n",
      "|    min| James|               29|            55000|\n",
      "|    25%|  NULL|               30|            60000|\n",
      "|    50%|  NULL|               32|            65000|\n",
      "|    75%|  NULL|               34|            70000|\n",
      "|    max|Robert|               37|            80000|\n",
      "+-------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "df.summary().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "19. How to calculate the number of characters in each word in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|len|\n",
      "+-----+---+\n",
      "| john|  4|\n",
      "|alice|  5|\n",
      "|  bob|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "df = df.withColumn('len', F.length(df.name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+-----------+\n",
      "|   Name|Age|Salary|first_diff|second_diff|\n",
      "+-------+---+------+----------+-----------+\n",
      "|  James| 34| 55000|      NULL|       NULL|\n",
      "| Robert| 37| 60000|      5000|       NULL|\n",
      "|    Jen| 32| 65000|      5000|          0|\n",
      "|Michael| 30| 70000|      5000|          0|\n",
      "|  Maria| 29| 80000|     10000|       5000|\n",
      "+-------+---+------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/23 15:58:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Difference of Differences\") \\\n",
    "    .getOrCreate()\n",
    "data = [('James', 34, 55000),\n",
    "        ('Michael', 30, 70000),\n",
    "        ('Robert', 37, 60000),\n",
    "        ('Maria', 29, 80000),\n",
    "        ('Jen', 32, 65000)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Salary\"])\n",
    "window_spec = Window.orderBy(\"Salary\")\n",
    "df_first_diff = df.withColumn(\"first_diff\", F.col(\"Salary\") - F.lag(\"Salary\").over(window_spec))\n",
    "df_second_diff = df_first_diff.withColumn(\"second_diff\", \n",
    "    F.col(\"first_diff\") - F.lag(\"first_diff\").over(window_spec))\n",
    "df_second_diff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. How to get the day of month, week number, day of year and day of week from a date strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|day_of_month|week_number|day_of_year|day_of_week|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|          18|         20|        138|          5|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|          31|         52|        365|          1|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"dateex\") \\\n",
    "    .getOrCreate()\n",
    "data = [(\"2023-05-18\", \"01 Jan 2010\"), (\"2023-12-31\", \"01 Jan 2010\")]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df = df.withColumn(\"date_1\", to_date(df.date_str_1, 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"date_2\", to_date(df.date_str_2, 'dd MMM yyyy'))\n",
    "\n",
    "df = df.withColumn(\"day_of_month\", dayofmonth(df.date_1))\\\n",
    ".withColumn(\"week_number\", weekofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_year\", dayofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_week\", dayofweek(df.date_1))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. How to convert year-month string to dates corresponding to the 4th day of the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|MonthYear|      Date|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-04|\n",
      "| Feb 2011|2011-02-04|\n",
      "| Mar 2012|2012-03-04|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "df = df.withColumn('Date', expr(\"to_date(MonthYear, 'MMM yyyy')\"))\n",
    "\n",
    "df = df.withColumn('Date', expr(\"date_add(date_sub(Date, day(Date) - 1), 3)\"))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 How to filter words that contain atleast 2 vowels from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "| Money|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "from pyspark.sql.functions import col, length, translate\n",
    "\n",
    "df2 = df.filter(F.col(\"word\").rlike(r'[aeiouAEIOU].*[aeiouAEIOU]'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. How to filter valid emails from a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|value            |\n",
      "+-----------------+\n",
      "|rameses@egypt.com|\n",
      "|matt@t.co        |\n",
      "|narendra@modi.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "pattern = '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'\n",
    "df2=df.filter(F.col(\"value\").rlike(pattern))\n",
    "df2.show(truncate =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. How to Pivot PySpark DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+------+\n",
      "|year|region|     1|     2|     3|     4|\n",
      "+----+------+------+------+------+------+\n",
      "|2021|    US|5000.0|5500.0|6000.0|7000.0|\n",
      "|2021|    EU|4000.0|4500.0|5000.0|6000.0|\n",
      "+----+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "pivot_df=df.groupBy(\"year\",\"region\").pivot(\"quarter\").agg(F.mean(\"revenue\"))\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. How to get the mean of a variable grouped by another variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|   Product|avg(Price)|\n",
      "+----------+----------+\n",
      "|    Laptop|    1100.0|\n",
      "|     Mouse|      40.0|\n",
      "|Smartphone|     700.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df2=df.groupBy(\"Product\").agg(F.mean(\"Price\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. How to compute the euclidean distance between two columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|series1|series2|euclidean_distance|\n",
      "+-------+-------+------------------+\n",
      "|      1|     10|               9.0|\n",
      "|      2|      9|               7.0|\n",
      "|      3|      8|               5.0|\n",
      "|      4|      7|               3.0|\n",
      "|      5|      6|               1.0|\n",
      "|      6|      5|               1.0|\n",
      "|      7|      4|               3.0|\n",
      "|      8|      3|               5.0|\n",
      "|      9|      2|               7.0|\n",
      "|     10|      1|               9.0|\n",
      "+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Euclideanex\") \\\n",
    "    .getOrCreate()\n",
    "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
    "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
    "df = df.withColumn(\"euclidean_distance\", F.sqrt((F.col(\"series1\") - F.col(\"series2\"))**2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. How to replace missing spaces in a string with the least frequent character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|string           |replaced_string  |\n",
      "+-----------------+-----------------+\n",
      "|dbc deb abed gade|dbccdebcabedcgade|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from collections import Counter\n",
    "\n",
    "df = spark.createDataFrame([('dbc deb abed gade',)], [\"string\"])\n",
    "def least_frequent_char(s):\n",
    "    s = s.replace(\" \", \"\")  \n",
    "    if not s:  \n",
    "        return None\n",
    "    return min(Counter(s), key=Counter(s).get)\n",
    "least_f = F.udf(least_frequent_char)\n",
    "least_char = df.select(least_f(F.col(\"string\")).alias(\"least_char\")).first()[0]\n",
    "if least_char:\n",
    "    df = df.withColumn(\"replaced_string\", F.regexp_replace(F.col(\"string\"), \" \", least_char))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      date|random_numbers|\n",
      "+----------+--------------+\n",
      "|2000-01-01|             5|\n",
      "|2000-01-08|             1|\n",
      "|2000-01-15|             9|\n",
      "|2000-01-22|             6|\n",
      "|2000-01-29|             3|\n",
      "|2000-02-05|             2|\n",
      "|2000-02-12|             3|\n",
      "|2000-02-19|             2|\n",
      "|2000-02-26|             6|\n",
      "|2000-03-04|             4|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, explode, sequence, rand\n",
    "\n",
    "# Start date and end date (start + 10 weekends)\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2000-03-04' \n",
    "df = spark.range(1).select(\n",
    "explode(\n",
    "sequence(\n",
    "expr(f\"date '{start_date}'\"),\n",
    "expr(f\"date '{end_date}'\"),\n",
    "expr(\"interval 1 day\")\n",
    ")\n",
    ").alias(\"date\")\n",
    ")\n",
    "df = df.filter(expr(\"dayofweek(date) = 7\")) \n",
    "df = df.withColumn(\"random_numbers\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. How to get the nrows, ncolumns, datatype of a dataframe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 17:58:07 WARN SparkContext: The path https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling.csv has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|summary|         RowNumber|       CustomerId|Surname|      CreditScore|Geography|Gender|               Age|            Tenure|          Balance|     NumOfProducts|          HasCrCard|     IsActiveMember|  EstimatedSalary|             Exited|\n",
      "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|  count|             10000|            10000|  10000|            10000|    10000| 10000|             10000|             10000|            10000|             10000|              10000|              10000|            10000|              10000|\n",
      "|   mean|            5000.5|  1.56909405694E7|   NULL|         650.5288|     NULL|  NULL|           38.9218|            5.0128|76485.88928799961|            1.5302|             0.7055|             0.5151|100090.2398809998|             0.2037|\n",
      "| stddev|2886.8956799071675|71936.18612274907|   NULL|96.65329873613035|     NULL|  NULL|10.487806451704587|2.8921743770496837|62397.40520238599|0.5816543579989917|0.45584046447513327|0.49979692845891815|57510.49281769821|0.40276858399486065|\n",
      "|    min|                 1|         15565701|  Abazu|              350|   France|Female|                18|                 0|              0.0|                 1|                  0|                  0|            11.58|                  0|\n",
      "|    max|             10000|         15815690| Zuyeva|              850|    Spain|  Male|                92|                10|        250898.09|                 4|                  1|                  1|        199992.48|                  1|\n",
      "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "\n",
      "Number of rows: 10000\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Get the number of rows and columns\n",
    "nrows = df.count()\n",
    "ncolumns = len(df.columns)\n",
    "\n",
    "# Get the data types\n",
    "data_types = df.dtypes\n",
    "\n",
    "# Get summary statistics\n",
    "summary_stats = df.describe().show()\n",
    "\n",
    "# Convert to NumPy array and list\n",
    "numpy_array = df.toPandas().values\n",
    "list_equiv = df.collect()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of rows: {nrows}\")\n",
    "print(f\"Number of columns: {ncolumns}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. How to rename a specific columns in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n",
      "| name|user_age|user_qty|\n",
      "+-----+--------+--------+\n",
      "|Alice|       1|      30|\n",
      "|  Bob|       2|      35|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
    "\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. How to check if a dataframe has any missing values and count of missing values in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 0, 'Value': 2, 'id': 2}\n",
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "has_missing = any(row.asDict().values() for row in missing.collect())\n",
    "\n",
    "missing_count = missing.collect()[0].asDict()\n",
    "print(missing_count)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 How to replace missing values of multiple numeric columns with the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1| 289|\n",
      "|   B|   3| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6| 289|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"A\", 1, None),\n",
    "    (\"B\", None, 123),\n",
    "    (\"B\", 3, 456),\n",
    "    (\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "mean_values = df.select([F.mean(column).alias(column) for column in [\"var1\", \"var2\"]]).collect()[0]\n",
    "\n",
    "df2 = df.na.fill({\n",
    "    \"var1\": mean_values[\"var1\"],\n",
    "    \"var2\": mean_values[\"var2\"]\n",
    "})\n",
    "\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. How to change the order of columns of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "|Age|First_Name|Last_Name|\n",
      "+---+----------+---------+\n",
      "| 30|      John|      Doe|\n",
      "| 25|      Jane|      Doe|\n",
      "| 22|     Alice|    Smith|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "new_order = [\"Age\", \"First_Name\", \"Last_Name\"]\n",
    "df = df.select(*new_order)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. How to format or suppress scientific notations in a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id| your_column|\n",
      "+---+------------+\n",
      "|  1|0.0000001230|\n",
      "|  2|0.0000234560|\n",
      "|  3|0.0003456780|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "# Determine the number of decimal places you want\n",
    "decimal_places = 10\n",
    "\n",
    "df = df.withColumn(\"your_column\", format_number(\"your_column\", decimal_places))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
